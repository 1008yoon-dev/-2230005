# ë¼ì´íŠ¸ ì•„ê¸°ë™ì§„ì´
# ============================================================
# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
# ============================================================
import urllib.request
import os
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    get_linear_schedule_with_warmup
)
from torch.optim import AdamW


# ============================================================
# 1. NSMC ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ratings_train.txt, ratings_test.txt)
# ============================================================
train_url = "https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt"
test_url  = "https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt"
train_path = "ratings_train.txt"
test_path  = "ratings_test.txt"

if not os.path.exists(train_path):
    print("NSMC train ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
    urllib.request.urlretrieve(train_url, filename=train_path)
    print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")

if not os.path.exists(test_path):
    print("NSMC test ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
    urllib.request.urlretrieve(test_url, filename=test_path)
    print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")


# ============================================================
# 2. ë°ì´í„° ë¡œë“œ ë° 'ë¼ì´íŠ¸ ë²„ì „' ìƒ˜í”Œë§
# ============================================================
print("\në°ì´í„° ë¡œë“œ ì¤‘...")
train_df = pd.read_table(train_path)   # id, document, label
test_df  = pd.read_table(test_path)

# ê²°ì¸¡ì¹˜ ì œê±°
train_df = train_df.dropna()
test_df  = test_df.dropna()

# ğŸ”¹ ê°€ë²¼ìš´ ë²„ì „: train 5,000ê°œ / test 5,000ê°œë§Œ ì‚¬ìš©
train_small = train_df.sample(5000, random_state=42)
test_small  = test_df.sample(5000,  random_state=42)

print(f"train_small í¬ê¸°: {len(train_small)}")
print(f"test_small  í¬ê¸°: {len(test_small)}")

X_train, X_valid, y_train, y_valid = train_test_split(
    train_small["document"],
    train_small["label"],
    test_size=0.2,
    random_state=42
)


# ============================================================
# 3. BERT(KcBERT) í† í¬ë‚˜ì´ì € / ëª¨ë¸ / ë””ë°”ì´ìŠ¤ ì„¤ì •
# ============================================================
MODEL_NAME = "beomi/kcbert-base"   # ëŒ“ê¸€/ë¦¬ë·° ë„ë©”ì¸ì— ì˜ ë§ëŠ” KcBERT

# ğŸ”¹ ê°€ë²¼ìš´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¸íŒ… (CPU ì¹œí™”)
MAX_LEN   = 64
BATCH_SIZE = 16
EPOCHS     = 1

print("\ní† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=2   # 0: ë¶€ì •, 1: ê¸ì •
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")


# ============================================================
# 4. Dataset / DataLoader ì •ì˜
# ============================================================
class NSMCDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts.tolist()
        self.labels = labels.tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])

        encoding = self.tokenizer(
            text,
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        item = {key: val.squeeze(0) for key, val in encoding.items()}
        item["labels"] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item


train_dataset = NSMCDataset(X_train, y_train, tokenizer, MAX_LEN)
valid_dataset = NSMCDataset(X_valid, y_valid, tokenizer, MAX_LEN)
test_dataset  = NSMCDataset(test_small["document"], test_small["label"], tokenizer, MAX_LEN)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)

print("\nDataLoader ì¤€ë¹„ ì™„ë£Œ.")
print(f"  train ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
print(f"  valid ë°°ì¹˜ ìˆ˜: {len(valid_loader)}")
print(f"  test  ë°°ì¹˜ ìˆ˜: {len(test_loader)}")


# ============================================================
# 5. í•™ìŠµ / ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜
# ============================================================
def train_one_epoch(model, data_loader, optimizer, scheduler, device):
    model.train()
    total_loss = 0.0

    print(f"[train_one_epoch ì‹œì‘] ë¯¸ë‹ˆë°°ì¹˜ ê°œìˆ˜: {len(data_loader)}")

    for step, batch in enumerate(data_loader):
        batch = {k: v.to(device) for k, v in batch.items()}

        outputs = model(**batch)
        loss = outputs.loss

        loss.backward()
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

        total_loss += loss.item()

        # ì§„í–‰ ìƒí™© ë¡œê·¸ (ê°€ë²¼ìš´ ë²„ì „ì´ë¼ step ê°„ê²© ì¤„ì„)
        if (step + 1) % 50 == 0:
            print(f"  [step {step+1}/{len(data_loader)}] loss = {loss.item():.4f}")

    avg_loss = total_loss / max(1, step + 1)
    print(f"[train_one_epoch ì¢…ë£Œ] í‰ê·  loss = {avg_loss:.4f}")
    return avg_loss


def predict(model, data_loader, device):
    model.eval()
    preds = []
    labels_list = []

    with torch.no_grad():
        for batch in data_loader:
            labels = batch["labels"]
            batch = {k: v.to(device) for k, v in batch.items()}

            outputs = model(**batch)
            logits = outputs.logits
            batch_preds = torch.argmax(logits, dim=1)

            preds.extend(batch_preds.cpu().numpy().tolist())
            labels_list.extend(labels.numpy().tolist())

    return preds, labels_list


# ============================================================
# 6. Optimizer / Scheduler ì„¤ì •
# ============================================================
optimizer = AdamW(model.parameters(), lr=2e-5)

total_steps = len(train_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=int(0.1 * total_steps),
    num_training_steps=total_steps
)


# ============================================================
# 7. í•™ìŠµ ë£¨í”„ (ë¼ì´íŠ¸ ë²„ì „: EPOCHS=1)
# ============================================================
for epoch in range(EPOCHS):
    print(f"\n===== Epoch {epoch+1}/{EPOCHS} =====")
    train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, device)

    val_preds, val_labels = predict(model, valid_loader, device)
    val_acc = accuracy_score(val_labels, val_preds)
    print(f"Validation Accuracy: {val_acc:.4f}")


# ============================================================
# 8. í…ŒìŠ¤íŠ¸ ì…‹ í‰ê°€ (ë¼ì´íŠ¸: test_small ê¸°ì¤€)
# ============================================================
test_preds, test_labels = predict(model, test_loader, device)
test_acc = accuracy_score(test_labels, test_preds)
print("\n===== Test ê²°ê³¼ (BERT ë¼ì´íŠ¸ ë²„ì „) =====")
print("Test Accuracy:", test_acc)
print("\nClassification Report:")
print(classification_report(test_labels, test_preds, digits=4))


# ============================================================
# 9. í•œ ê°œ ë¦¬ë·° ì˜ˆì¸¡ í•¨ìˆ˜ (í™•ë¥ ê¹Œì§€ ë°˜í™˜)
# ============================================================
def predict_single_review(text, model, tokenizer, max_len, device):
    model.eval()

    with torch.no_grad():
        encoding = tokenizer(
            text,
            max_length=max_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        encoding = {k: v.to(device) for k, v in encoding.items()}

        outputs = model(**encoding)
        logits = outputs.logits          # [1, 2]
        probs_tensor = torch.softmax(logits, dim=1)[0]

        probs = probs_tensor.cpu().tolist()          # [p_neg, p_pos]
        pred_label = int(torch.argmax(probs_tensor).item())

    return pred_label, probs


# ============================================================
# 10. ê¸ì • í™•ë¥  â†’ 0~5ì  (0.5 ë‹¨ìœ„) ë³„ì ìœ¼ë¡œ ë³€í™˜
# ============================================================
def prob_to_star_rating(p_pos: float) -> float:
    """
    p_pos: ê¸ì • í™•ë¥  (0.0 ~ 1.0)
    ë°˜í™˜: 0.0 ~ 5.0 ì‚¬ì´ 0.5 ë‹¨ìœ„ ë³„ì 
    """
    raw_score = p_pos * 5.0                 # 0~5 ì‹¤ìˆ˜ê°’
    half_step = round(raw_score * 2) / 2.0  # 0.5 ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
    return max(0.0, min(5.0, half_step))    # 0~5 ë²”ìœ„ë¡œ í´ë¨í”„


# ============================================================
# 11. ì‹¤ì‹œê°„ ë¦¬ë·° ì…ë ¥ â†’ 0~5ì  ë³„ì  ì˜ˆì¸¡ê¸°
# ============================================================
print("\nğŸ”¹ ì‹¤ì‹œê°„ ë¦¬ë·° ë³„ì  ì˜ˆì¸¡ê¸° (0~5ì , 0.5 ë‹¨ìœ„)")
print("   ë¦¬ë·°ë¥¼ ì…ë ¥í•˜ë©´ BERTê°€ ë³„ì ì„ ë§¤ê²¨ì¤ë‹ˆë‹¤. (ì¢…ë£Œ: q)")

while True:
    text = input("\në¦¬ë·°ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ q): ").strip()
    if text.lower() == "q":
        print("ì¢…ë£Œí•©ë‹ˆë‹¤. ì˜¤ëŠ˜ë„ ê³ ìƒ ë§ì•˜ì–´ ë‘ëª© ğŸ’š")
        break

    if not text:
        print("ê³µë°± ë§ê³  ë‚´ìš©ì„ ì¢€ ì¨ì¤˜ìš” ë‘ëª© ğŸ¥º")
        continue

    pred_label, probs = predict_single_review(text, model, tokenizer, MAX_LEN, device)
    p_neg, p_pos = probs[0], probs[1]

    rating = prob_to_star_rating(p_pos)

    print(f"\n[ëª¨ë¸ ì˜ˆì¸¡]")
    print(f"  ë¶€ì • í™•ë¥  = {p_neg:.3f}, ê¸ì • í™•ë¥  = {p_pos:.3f}")
    print(f"  â†’ ì˜ˆì¸¡ ë³„ì  = â­ {rating:.1f} / 5.0")

    if pred_label == 1:
        print("  (ì „ì²´ íŒë‹¨: ê¸ì • ìª½ì— ê°€ê¹ìŠµë‹ˆë‹¤.)")
    else:
        print("  (ì „ì²´ íŒë‹¨: ë¶€ì • ìª½ì— ê°€ê¹ìŠµë‹ˆë‹¤.)")
